{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Documentos</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\", \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\", \"Human machine interface for machine learning applications\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etapa de procesamiento Â¿Que se hace?\n",
    "tok_docs = [] #Documentos procesados\n",
    "vocab = [] #Vocabulario, muy importante!\n",
    "\n",
    "stop_words = set(stopwords.words('english')) #Lista en Ingles\n",
    "for doc in documents:\n",
    "    word_tok = nltk.word_tokenize(doc)\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tok: \n",
    "        w = w.lower()\n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "            vocab.append(w)\n",
    "    tok_docs.append(filtered_sentence)\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generation', 'quasi', 'error', 'unordered', 'paths', 'abc', 'widths', 'minors', 'graph', 'ordering', 'binary', 'survey', 'relation', 'measurement', 'perceived', 'random', 'human', 'response', 'management', 'computer', 'well', 'trees', 'lab', 'opinion', 'time', 'user', 'iv', 'interface', 'eps', 'machine', 'applications', 'testing', 'intersection', 'engineering', 'system', 'learning'}\n",
      "36\n",
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey'], ['human', 'machine', 'interface', 'machine', 'learning', 'applications']]\n"
     ]
    }
   ],
   "source": [
    "#Vocabulario y documentos despues de procesamiento\n",
    "print(vocab)\n",
    "print(len(vocab))\n",
    "print(tok_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Esta funcion crea el diccionario TF de cada documento\n",
    "def computeDocsTFDict(doc):\n",
    "    \"\"\" Retorna un diccionario de frecuencias  \n",
    "    con las palabras unicas del documento.\n",
    "    \"\"\"\n",
    "    #Counts the number of times the word appears in review\n",
    "    TFDict = {}\n",
    "    for word in doc:\n",
    "        if word in TFDict:\n",
    "            TFDict[word] += 1\n",
    "        else:\n",
    "            TFDict[word] = 1\n",
    "    #Computes tf for each word           \n",
    "    for word in TFDict:\n",
    "        TFDict[word] = TFDict[word] / len(doc)\n",
    "    return TFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 0.14285714285714285,\n",
       " 'machine': 0.14285714285714285,\n",
       " 'interface': 0.14285714285714285,\n",
       " 'lab': 0.14285714285714285,\n",
       " 'abc': 0.14285714285714285,\n",
       " 'computer': 0.14285714285714285,\n",
       " 'applications': 0.14285714285714285}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfDict = [] #En este diccionario se va a almacenar el conteo\n",
    "for doc in tok_docs:\n",
    "    tfDict.append(computeDocsTFDict(doc))\n",
    "tfDict[0] #El indice entre tfDict y tok_docs corresponden a los mismos documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculo de IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 3,\n",
       " 'machine': 2,\n",
       " 'interface': 3,\n",
       " 'lab': 1,\n",
       " 'abc': 1,\n",
       " 'computer': 2,\n",
       " 'applications': 2,\n",
       " 'survey': 2,\n",
       " 'user': 3,\n",
       " 'opinion': 1,\n",
       " 'system': 3,\n",
       " 'response': 2,\n",
       " 'time': 2,\n",
       " 'eps': 2,\n",
       " 'management': 1,\n",
       " 'engineering': 1,\n",
       " 'testing': 1,\n",
       " 'relation': 1,\n",
       " 'perceived': 1,\n",
       " 'error': 1,\n",
       " 'measurement': 1,\n",
       " 'generation': 1,\n",
       " 'random': 1,\n",
       " 'binary': 1,\n",
       " 'unordered': 1,\n",
       " 'trees': 3,\n",
       " 'intersection': 1,\n",
       " 'graph': 3,\n",
       " 'paths': 1,\n",
       " 'minors': 2,\n",
       " 'iv': 1,\n",
       " 'widths': 1,\n",
       " 'well': 1,\n",
       " 'quasi': 1,\n",
       " 'ordering': 1,\n",
       " 'learning': 1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para el calculo de IDF necesitamos conocer primero cuantas veces una palabra aparace en los documentos del corpus\n",
    "def computeWordCountDict(tfDict):\n",
    "    \"\"\" \n",
    "    Devuelve un diccionario cuyos indices son todas las palabras unicas en el conjunto de datos y cuyos valores cuentan el numero de\n",
    "    documentos en las que aparece la palabra\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "\n",
    "    for doc in tfDict:\n",
    "        for word in doc:\n",
    "            if word in countDict:\n",
    "                countDict[word] += 1\n",
    "            else:\n",
    "                countDict[word] = 1\n",
    "    return countDict\n",
    "\n",
    "countDict = computeWordCountDict(tfDict)\n",
    "countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 1.2039728043259361,\n",
       " 'machine': 1.6094379124341003,\n",
       " 'interface': 1.2039728043259361,\n",
       " 'lab': 2.302585092994046,\n",
       " 'abc': 2.302585092994046,\n",
       " 'computer': 1.6094379124341003,\n",
       " 'applications': 1.6094379124341003,\n",
       " 'survey': 1.6094379124341003,\n",
       " 'user': 1.2039728043259361,\n",
       " 'opinion': 2.302585092994046,\n",
       " 'system': 1.2039728043259361,\n",
       " 'response': 1.6094379124341003,\n",
       " 'time': 1.6094379124341003,\n",
       " 'eps': 1.6094379124341003,\n",
       " 'management': 2.302585092994046,\n",
       " 'engineering': 2.302585092994046,\n",
       " 'testing': 2.302585092994046,\n",
       " 'relation': 2.302585092994046,\n",
       " 'perceived': 2.302585092994046,\n",
       " 'error': 2.302585092994046,\n",
       " 'measurement': 2.302585092994046,\n",
       " 'generation': 2.302585092994046,\n",
       " 'random': 2.302585092994046,\n",
       " 'binary': 2.302585092994046,\n",
       " 'unordered': 2.302585092994046,\n",
       " 'trees': 1.2039728043259361,\n",
       " 'intersection': 2.302585092994046,\n",
       " 'graph': 1.2039728043259361,\n",
       " 'paths': 2.302585092994046,\n",
       " 'minors': 1.6094379124341003,\n",
       " 'iv': 2.302585092994046,\n",
       " 'widths': 2.302585092994046,\n",
       " 'well': 2.302585092994046,\n",
       " 'quasi': 2.302585092994046,\n",
       " 'ordering': 2.302585092994046,\n",
       " 'learning': 2.302585092994046}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para el calculo de IDF necesitamos conocer primero cuantas veces una palabra aparace en los documentos del corpus\n",
    "def computeIDFDict(countDict):\n",
    "    \"\"\" Devuelve un diccionario cuyos indices son palabras \n",
    "        y sus valores son el idf correspondiente.\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    for word in countDict:\n",
    "        idfDict[word] = math.log(len(documents) / countDict[word])\n",
    "    return idfDict\n",
    "  \n",
    "idfDict = computeIDFDict(countDict)\n",
    "idfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculo de TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 0.17199611490370514,\n",
       " 'machine': 0.22991970177630003,\n",
       " 'interface': 0.17199611490370514,\n",
       " 'lab': 0.32894072757057796,\n",
       " 'abc': 0.32894072757057796,\n",
       " 'computer': 0.22991970177630003,\n",
       " 'applications': 0.22991970177630003}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeDocsTFIDFDict(TFDict,idfDict):\n",
    "    \"\"\" Devuelve un diccionario cuyas claves son todas las palabras unicas en\n",
    "    la revision y cuyos valores son su tfidf correspondiente.\n",
    "    \"\"\"\n",
    "    TFIDFDict = {}\n",
    "    for word in TFDict:\n",
    "        TFIDFDict[word] = TFDict[word] * idfDict[word]\n",
    "    return TFIDFDict\n",
    "\n",
    "tfidfDict = [computeDocsTFIDFDict(doc,idfDict) for doc in tfDict]\n",
    "tfidfDict[0] #El indice entre tfidfDict y tok_docs corresponden a los mismos documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorizacion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.32894072757057796,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.17199611490370514,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.22991970177630003,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.32894072757057796,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.17199611490370514,\n",
       " 0.0,\n",
       " 0.22991970177630003,\n",
       " 0.22991970177630003,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vamos a utilizar el vocabulario alamacenado en vocab como dimensiones\n",
    "def computeTFIDFVector(doc):\n",
    "    tfidfVector = [0.0] * len(vocab) #Vector del tamano del vocabulario\n",
    "    # Para cada palabra unica, si esta en el documento, se almacena su valor TF-IDF.\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in doc:\n",
    "            tfidfVector[i] = doc[word]\n",
    "    return tfidfVector\n",
    "\n",
    "tfidfVector = [computeTFIDFVector(doc) for doc in tfidfDict]\n",
    "tfidfVector[0] #Vector que representa el primer documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Similitud Coseno</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(vector_x, vector_y):\n",
    "    dot = 0.0\n",
    "    for e_x, e_y in zip(vector_x, vector_y):\n",
    "       dot += e_x * e_y\n",
    "    return dot\n",
    "\n",
    "def magnitude(vector):\n",
    "    mag = 0.0\n",
    "    for index in vector:\n",
    "        mag += math.pow(index, 2)\n",
    "    return math.sqrt(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04937924273636205"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_similarity_0_1 = dot_product(tfidfVector[0], tfidfVector[1])/ magnitude(tfidfVector[0]) * magnitude(tfidfVector[1])\n",
    "doc_similarity_0_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tarea: Buscar los documentos mas similares entre si en nuestro corpus</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
